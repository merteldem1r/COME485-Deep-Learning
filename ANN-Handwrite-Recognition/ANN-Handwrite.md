# Complete Step-by-Step Explanation of ann_for_mnist.ipynb

**NOTE**: Explanations generated by AI for better understanding

## Table of Contents
1. [Overview and Purpose](#overview-and-purpose)
2. [Setup and Styling](#setup-and-styling)
3. [Importing Libraries](#importing-libraries)
4. [Loading the MNIST Dataset](#loading-the-mnist-dataset)
5. [Understanding Data Structure](#understanding-data-structure)
6. [Creating Data Loaders](#creating-data-loaders)
7. [Visualizing Sample Data](#visualizing-sample-data)
8. [Building the Neural Network](#building-the-neural-network)
9. [Training Configuration](#training-configuration)
10. [Understanding Input Format](#understanding-input-format)
11. [The Training Process](#the-training-process)
12. [Visualizing Training Results](#visualizing-training-results)
13. [Final Model Evaluation](#final-model-evaluation)
14. [Testing with Custom Handwritten Digits](#testing-with-custom-handwritten-digits)
15. [Key Lessons and Debugging Journey](#key-lessons-and-debugging-journey)

---

## Overview and Purpose

This notebook demonstrates how to build, train, and use an **Artificial Neural Network (ANN)** using PyTorch to recognize handwritten digits. The notebook uses the famous **MNIST dataset**, which contains 70,000 images of handwritten digits (0-9).

**Main Goals:**
- Learn PyTorch basics for deep learning
- Build a multilayer perceptron (fully connected neural network)
- Train the model to recognize digits with high accuracy
- Test the model on custom handwritten digit images

---

## Setup and Styling

**First Cell: CSS Styling**

```python
my_css = """..."""
open('style.css', 'w').write(my_css)
```

**What happens here:**
- Creates custom CSS styling for the notebook to make it visually appealing
- Defines styles for headers (h1, h2) with blue gradients and borders
- Writes these styles to a `style.css` file
- Applies the styling to the notebook using IPython's HTML display

**Why it matters:** Makes the notebook more readable and professional-looking, but doesn't affect the machine learning functionality.

---

## Importing Libraries

**Cell 3: PyTorch Core Libraries**

```python
import torch
from torch import nn
from torch.nn import functional as F
from torch.utils.data import DataLoader
from torchvision import datasets
import torchvision.transforms as T
```

**What each import does:**

- **`torch`**: Main PyTorch library for tensor operations (like NumPy but for deep learning)
- **`nn`**: Neural network module containing building blocks (layers, loss functions)
- **`functional as F`**: Pre-built activation functions (ReLU, softmax, etc.)
- **`DataLoader`**: Utility to load data in batches and shuffle it during training
- **`datasets`**: Pre-built datasets including MNIST
- **`transforms as T`**: Tools to transform images (resize, normalize, convert to tensors)

**Cell 4: Utility Libraries**

```python
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
```

- **`numpy`**: For numerical operations and array manipulation
- **`matplotlib`**: For plotting and visualizing images/results
- **`tqdm`**: Creates progress bars during training (shows how many epochs completed)

---

## Loading the MNIST Dataset

**Understanding MNIST:**

The **MNIST dataset** (Modified National Institute of Standards and Technology) is a collection of handwritten digit images used as a standard benchmark in machine learning.

**Dataset Statistics:**
- **Training set**: 60,000 images
- **Test set**: 10,000 images
- **Image size**: 28Ã—28 pixels (784 pixels total)
- **Color**: Grayscale (1 channel)
- **Labels**: Digits 0-9

**Cell 7: Downloading and Loading Data**

```python
mytransform = T.ToTensor()
train_data = datasets.MNIST(root = './', download=True, train = True, transform = mytransform)
test_data = datasets.MNIST(root = './', download=True, train = False, transform = mytransform)
```

**Step-by-step breakdown:**

1. **`T.ToTensor()`**: Creates a transformer that converts images (3D arrays) to PyTorch tensors
   - Converts pixel values from 0-255 to 0-1 (normalization)
   - Rearranges dimensions to PyTorch format (Channel, Height, Width)

2. **`datasets.MNIST()`**: Downloads and loads MNIST dataset
   - **`root = './'`**: Save dataset in current directory
   - **`download=True`**: Download if not already present
   - **`train = True/False`**: Load training or test set
   - **`transform = mytransform`**: Apply the ToTensor transformation

**What gets downloaded:**
- Four files stored in `MNIST/raw/` folder:
  - `train-images-idx3-ubyte` (training images)
  - `train-labels-idx1-ubyte` (training labels)
  - `t10k-images-idx3-ubyte` (test images)
  - `t10k-labels-idx1-ubyte` (test labels)

---

## Understanding Data Structure

**Cell 9: Examining Image Shape**

```python
img, label = train_data[0]
img.shape  # Returns: torch.Size([1, 28, 28])
```

**What this shows:**
- Each image is a **3D tensor**: `(Channels, Height, Width)`
- **1 channel** because it's grayscale (RGB would have 3 channels)
- **28Ã—28 pixels** = 784 total pixel values

**Cell 11: Visualizing First Image**

```python
plt.imshow(img.reshape(28,28), cmap = 'gist_yarg')
plt.axis('off')
```

**What happens:**
- **`reshape(28,28)`**: Converts 3D tensor to 2D array (removes channel dimension)
- **`cmap='gist_yarg'`**: Color map that inverts black and white (better visualization)
- **`axis('off')`**: Hides the axis numbers
- **Result**: Displays the first training image (usually a "5")

---

## Creating Data Loaders

**Why We Need DataLoader:**

Neural networks don't process all data at once. They learn from small batches of data iteratively. The DataLoader automatically:
- Divides data into batches
- Shuffles data (prevents learning order patterns)
- Handles parallel loading

**Cell 13: Creating Loaders**

```python
torch.manual_seed(101)
train_loader = DataLoader(train_data, batch_size = 100, shuffle=True)
test_loader = DataLoader(test_data, batch_size = 500, shuffle=False)
```

**Understanding each parameter:**

1. **`torch.manual_seed(101)`**: Sets random seed for reproducibility
   - Same seed = same random shuffling every time you run
   - Makes experiments repeatable

2. **Training Loader:**
   - **`batch_size = 100`**: Process 100 images at a time
   - **`shuffle=True`**: Randomly shuffle training data each epoch
   - **Total batches**: 60,000 Ã· 100 = 600 batches per epoch

3. **Test Loader:**
   - **`batch_size = 500`**: Larger batches (no backpropagation needed)
   - **`shuffle=False`**: No need to shuffle test data
   - **Total batches**: 10,000 Ã· 500 = 20 batches

**Cell 15: Custom Loader Class**

```python
@dataclass
class Loader:
    train: DataLoader
    test: DataLoader

myloader = Loader(train = train_loader, test = test_loader)
```

**Purpose:**
- Combines train and test loaders into one object
- Access via `myloader.train` and `myloader.test`
- Alternative to using a dictionary (personal preference)

---

## Visualizing Sample Data

**Cell 17: Getting One Batch**

```python
for img, label in myloader.train:
    break  # Run only one iteration
img.shape  # Returns: torch.Size([100, 1, 28, 28])
```

**What this does:**
- Iterates through training loader but immediately breaks
- Gets one batch: 100 images with their labels
- Shape: `[batch_size, channels, height, width]`

**Cell 23: Plotting 50 Images**

```python
fig, ax = plt.subplots(nrows = 5, ncols = 10, figsize=(8,4))
for row in range(0,5):
    for col in range(0,10):
        myid = (10*row) + col
        ax[row,col].imshow(myimages[myid].transpose(1,2,0), cmap = 'gist_yarg')
```

**Step-by-step:**
1. Create a 5Ã—10 grid of subplots (50 total)
2. Loop through each position (row, col)
3. Calculate image index: `myid = 10*row + col`
4. **Transpose**: Convert from `(C,H,W)` to `(H,W,C)` (matplotlib requirement)
5. Display each image with inverted grayscale

**Why transpose?**
- PyTorch uses: (Channel, Height, Width)
- Matplotlib expects: (Height, Width, Channel)
- `transpose(1,2,0)` rearranges dimensions

---

## Building the Neural Network

**Cell 24: Network Architecture**

```python
class MultilayerPerceptron(nn.Module):
    def __init__(self, in_features = 784, out_features=10):
        super(MultilayerPerceptron, self).__init__()
        
        self.fc1 = nn.Linear(in_features, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, out_features)
    
    def forward(self, X):
        X = F.relu(self.fc1(X))
        X = F.relu(self.fc2(X))
        X = self.fc3(X)
        return F.log_softmax(X, dim = 1)
```

**Complete Architecture Breakdown:**

### Layer Structure:

**Input Layer (784 neurons):**
- Each neuron receives one pixel value (28Ã—28 = 784 pixels)
- Pixel values range from 0.0 (black) to 1.0 (white)

**First Hidden Layer (120 neurons):**
- **`self.fc1 = nn.Linear(784, 120)`**: Fully connected layer
- Each of the 120 neurons connects to all 784 input neurons
- **Weights**: 784 Ã— 120 = 94,080 connections
- **Biases**: 120 (one per neuron)
- **Activation**: ReLU (Rectified Linear Unit)

**Second Hidden Layer (84 neurons):**
- **`self.fc2 = nn.Linear(120, 84)`**: Connects to previous layer
- Each of the 84 neurons connects to all 120 previous neurons
- **Weights**: 120 Ã— 84 = 10,080 connections
- **Biases**: 84
- **Activation**: ReLU

**Output Layer (10 neurons):**
- **`self.fc3 = nn.Linear(84, 10)`**: Final layer
- 10 neurons (one for each digit 0-9)
- **Weights**: 84 Ã— 10 = 840 connections
- **Biases**: 10
- **Activation**: Log Softmax (converts to probabilities)

### Understanding the Activations:

**ReLU (Rectified Linear Unit):**
```
ReLU(x) = max(0, x)
```
- If input is negative: output = 0
- If input is positive: output = input (unchanged)
- **Why use it?** Prevents vanishing gradient problem, faster training
- **Example**: ReLU(-5) = 0, ReLU(3.2) = 3.2

**Log Softmax:**
```
log_softmax(x_i) = log(exp(x_i) / Î£ exp(x_j))
```
- Converts raw scores to log probabilities
- All outputs sum to 1 (when exponentiated)
- **Why log?** Numerical stability and works with CrossEntropyLoss
- **Example output**: [-0.1, -2.5, -3.8, ...] for 10 digits

### Total Parameters:

**Weights:**
- Layer 1: 784 Ã— 120 = 94,080
- Layer 2: 120 Ã— 84 = 10,080
- Layer 3: 84 Ã— 10 = 840
- **Total weights**: 105,000

**Biases:**
- Layer 1: 120
- Layer 2: 84
- Layer 3: 10
- **Total biases**: 214

**Grand Total: 105,214 learnable parameters**

**Cell 26: Creating Model Instance**

```python
torch.manual_seed(101)
mymodel = MultilayerPerceptron()
```

- Sets random seed (reproducible weight initialization)
- Creates the model with default parameters (784 input, 10 output)
- Initializes all 105,214 parameters with small random values

---

## Training Configuration

**Cell 28: Loss Function and Optimizer**

```python
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(mymodel.parameters(), lr = 1e-3)
```

### Understanding the Loss Function:

**Cross-Entropy Loss:**
- Measures the difference between predicted probabilities and true labels
- Formula: `L = -Î£ y_true * log(y_pred)`
- **Lower loss** = better predictions
- **Why use it?** Ideal for multi-class classification problems

**Example:**
- True label: digit "3" (one-hot: [0,0,0,1,0,0,0,0,0,0])
- Predicted: [0.05, 0.05, 0.1, 0.6, 0.1, 0.05, 0.02, 0.01, 0.01, 0.01]
- If prediction for "3" is high (0.6) â†’ low loss
- If prediction for "3" is low (0.1) â†’ high loss

### Understanding the Optimizer:

**Adam (Adaptive Moment Estimation):**
- Advanced version of gradient descent
- Automatically adjusts learning rate for each parameter
- **`lr = 1e-3`** (0.001): Learning rate (step size)
- **How it works**:
  1. Calculate gradient (how to adjust each weight)
  2. Use momentum (average of past gradients)
  3. Adapt learning rate based on gradient history
  4. Update weights: `weight = weight - learning_rate * gradient`

**Why Adam?**
- Faster convergence than basic gradient descent
- Automatically handles different parameter scales
- Good default choice for most problems

---

## Understanding Input Format

**Cell 33: Checking Batch Shape**

```python
myiter = iter(myloader.train)
img, label = next(myiter)
img.shape  # torch.Size([100, 1, 28, 28])
```

**Problem identified:**
- DataLoader gives: `[batch_size, channels, height, width]` = `[100, 1, 28, 28]`
- Our model expects: `[batch_size, features]` = `[100, 784]`
- **Solution needed**: Flatten the 3D images into 1D vectors

**Cell 35: Flattening Images**

```python
img.view(100,-1).shape  # torch.Size([100, 784])
```

**Understanding `.view()`:**
- Reshapes tensor without copying data
- **`100`**: Keep batch size as 100
- **`-1`**: Automatically calculate remaining dimension (1Ã—28Ã—28 = 784)
- **Result**: Each image becomes a 784-element vector

**Visual representation:**
```
Before flattening: [100, 1, 28, 28]
[
  [[pixel_row_1],
   [pixel_row_2],
   ...
   [pixel_row_28]]  â† One 28Ã—28 image
  ... (99 more images)
]

After flattening: [100, 784]
[
  [pix1, pix2, pix3, ..., pix784],  â† One flattened image
  [pix1, pix2, pix3, ..., pix784],
  ... (98 more)
]
```

**Cell 37: Testing Untrained Model**

```python
y_pred = mymodel(img.view(100,-1))
y_pred.shape  # torch.Size([100, 10])
```

**What happens:**
- Model processes 100 images
- Returns 100 predictions, each with 10 probabilities (one per digit)
- **Before training**: Random predictions (untrained weights)

**Cell 39: Getting Predicted Digits**

```python
val, idx = torch.max(y_pred, dim=1)
```

- **`torch.max(..., dim=1)`**: Find maximum probability along dimension 1
- **`val`**: The maximum probability value for each image
- **`idx`**: The index (0-9) of that maximum = predicted digit
- **Example**: If idx[0] = 7, the model predicts image 0 is digit "7"

---

## The Training Process

**Cell 41: Tracking Variables**

```python
class Loss:
    train: list = []
    test: list = []

class Accuracy:
    train: list = []
    test: list = []
```

**Purpose:**
- Store loss and accuracy for each epoch
- Allows us to plot training progress later
- Separate tracking for training and test sets

**Cell 42: The Main Training Loop**

This is the heart of the notebook. Let's break it down step by step:

```python
epochs = 10
for epoch in tqdm(range(epochs)):
```

**Epoch**: One complete pass through all training data
- Training happens over 10 epochs
- Each epoch processes all 60,000 training images

### Training Phase (Learning):

```python
train_corr = 0
for batch, (img, label) in enumerate(myloader.train):
    batch += 1
    y_pred = mymodel(img.view(100,-1))
    loss = criterion(y_pred, label)
```

**Step-by-step process:**

1. **Initialize counter**: `train_corr = 0` (counts correct predictions)

2. **Loop through batches**: 600 iterations per epoch (60,000 Ã· 100)

3. **Forward Pass**:
   - `img.view(100,-1)`: Flatten 100 images to 784 pixels each
   - `mymodel(...)`: Feed through network
   - **Process**:
     - Input (784) â†’ Layer 1 (120 neurons) â†’ ReLU
     - Layer 1 (120) â†’ Layer 2 (84 neurons) â†’ ReLU
     - Layer 2 (84) â†’ Output (10 neurons) â†’ Log Softmax
   - Output: 100Ã—10 matrix of log probabilities

4. **Calculate Loss**:
   - `criterion(y_pred, label)`: Compare predictions to true labels
   - Returns a single number representing prediction error
   - **Lower = better**

```python
_, prediction = torch.max(y_pred, dim=1)
train_corr += (prediction == label).sum()
```

5. **Calculate Accuracy**:
   - `torch.max(y_pred, dim=1)`: Get predicted digit for each image
   - `prediction == label`: Boolean array (True = correct, False = wrong)
   - `.sum()`: Count how many are correct
   - Accumulate correct predictions across batches

### The Learning Part (Backpropagation):

```python
optimizer.zero_grad()
loss.backward()
optimizer.step()
```

**This is where the magic happens:**

1. **`optimizer.zero_grad()`**:
   - Clears previous gradients
   - **Why?** PyTorch accumulates gradients; we need fresh ones each batch

2. **`loss.backward()`**:
   - **Backpropagation**: Calculates how to adjust each of 105,214 parameters
   - Computes gradient (derivative) of loss with respect to each weight
   - **Formula**: âˆ‚Loss/âˆ‚weight for every weight
   - **Result**: Each parameter knows "increase me" or "decrease me" to reduce loss

3. **`optimizer.step()`**:
   - Updates all parameters using calculated gradients
   - **Formula**: `weight_new = weight_old - learning_rate * gradient`
   - **Example**: 
     - If gradient = +0.5, weight decreases by 0.001 Ã— 0.5 = 0.0005
     - If gradient = -0.3, weight increases by 0.001 Ã— 0.3 = 0.0003

### Monitoring Progress:

```python
if batch % 200 == 0:
    acc = 100 * (train_corr.item() / (batch*100))
    print(f'Epoch:{epoch:2d} batch: {batch:2d} loss: {loss.item():4.4f} Accuracy: {acc:4.4f} %')
```

**Every 200 batches** (3 times per epoch):
- Calculate current accuracy
- Print progress: epoch number, batch number, loss, accuracy
- **Example output**: `Epoch: 0 batch:200 loss: 0.3456 Accuracy: 87.25 %`

### Storing Epoch Results:

```python
Loss.train.append(loss.item())
accuracy = 100 * (train_corr.item() / (batch*100))
Accuracy.train.append(accuracy)
```

- Save final loss and accuracy for this epoch
- Used later for plotting training curves

### Validation Phase (Testing):

```python
with torch.no_grad():
    for batch, (img, label) in enumerate(myloader.test):
        batch += 1
        y_val = mymodel(img.view(500,-1))
        _, predicted = torch.max(y_val, dim=1)
        test_corr += (predicted == label).sum()
```

**Why validate?**
- Check if model generalizes to unseen data
- Detect overfitting (when training accuracy >> test accuracy)

**Key difference from training:**

1. **`with torch.no_grad()`**:
   - Disables gradient calculation
   - **Why?** We're not learning, just evaluating
   - Saves memory and speeds up computation

2. **Larger batches** (500 instead of 100):
   - No backpropagation needed, so we can process more at once
   - 20 batches total (10,000 Ã· 500)

3. **No optimizer calls**:
   - No `zero_grad()`, `backward()`, or `step()`
   - Weights don't change during validation

**What happens each epoch:**
1. Train on 60,000 images (adjusting weights)
2. Test on 10,000 images (checking performance)
3. Store results
4. Repeat for next epoch

**Total iterations:**
- 10 epochs Ã— 600 training batches = 6,000 training iterations
- 10 epochs Ã— 20 test batches = 200 validation iterations

---

## Visualizing Training Results

**Cell 44: Plotting Loss and Accuracy**

```python
fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12,4))
ax[0].plot(Loss.train, label = 'Training')
ax[0].plot(Loss.test, label='test/validation')
ax[1].plot(Accuracy.train, label = 'Training')
ax[1].plot(Accuracy.test, label='test/validation')
```

**What we see:**

### Left Plot - Loss Over Epochs:

**Training Loss:**
- Starts high (~0.5-0.6)
- Decreases rapidly in first few epochs
- Continues decreasing gradually
- **Interpretation**: Model is learning, errors decreasing

**Test Loss:**
- Follows similar pattern
- Slightly higher than training loss (expected)
- **Good sign**: Both decreasing together

### Right Plot - Accuracy Over Epochs:

**Training Accuracy:**
- Starts around 85-90%
- Increases steadily
- Reaches ~99% by epoch 10
- **Shows**: Model memorizing training data well

**Test Accuracy:**
- Starts around 85-90%
- Increases but plateaus around 97-98%
- **Critical**: Slightly lower than training (normal)
- **Plateau**: Model reached its learning capacity

**Understanding the Gap:**
- Training accuracy > Test accuracy is normal
- **Small gap (1-2%)**: Good generalization
- **Large gap (>5%)**: Overfitting (memorizing instead of learning)
- Our gap: ~1.5% â†’ Healthy model!

**Key Observation from Plot:**
- Gray dotted line at epoch 2: Suggested optimal stopping point
- After epoch 2-3, test accuracy plateaus
- Training continues improving but test doesn't
- **Lesson**: More epochs don't always mean better real-world performance

---

## Final Model Evaluation

**Cell 46: Loading All Test Data**

```python
test_loader = DataLoader(test_data, batch_size = 10_000, shuffle=False)
myiter = iter(test_loader)
img, label = next(myiter)
```

**Why reload?**
- Previous test_loader used batch_size=500
- Now we want all 10,000 test images in one batch
- Easier to create confusion matrix with all data at once

**Cell 47: Final Accuracy Calculation**

```python
with torch.no_grad():
    correct = 0
    for X, y_label in test_loader:
        y_val = mymodel(X.view(X.shape[0],-1))
        _, predicted = torch.max(y_val, dim=1)
        correct += (predicted == y_label).sum()

print(f'Test accuracy: = {correct.item()*100/(len(test_data)):2.4f} %')
```

**Step-by-step:**
1. Disable gradient calculation (evaluation mode)
2. Process all 10,000 test images
3. Count correct predictions
4. Calculate percentage: (correct / 10,000) Ã— 100
5. **Typical result**: ~97.59% accuracy

**What 97.59% means:**
- Out of 10,000 images, ~9,759 predicted correctly
- Only ~241 errors
- **Excellent performance** for a simple neural network!

**Cell 48: Confusion Matrix**

```python
from sklearn.metrics import confusion_matrix
confusion_array = confusion_matrix(y_true = y_label, y_pred = predicted)
```

**What is a confusion matrix?**

A 10Ã—10 table showing predictions vs. true labels:

```
         Predicted
         0    1    2    3    4    5    6    7    8    9
True 0 [972    0    1    0    0    2    3    1    1    0]
     1 [  0 1126    3    1    0    1    2    1    1    0]
     2 [  4    2  998    4    2    0    2    8   12    0]
     3 [  0    0    4  987    0    7    0    6    5    1]
     ...
```

**Reading the matrix:**
- **Diagonal** (top-left to bottom-right): Correct predictions
- **Off-diagonal**: Errors
- **Example**: Row 8, Column 3 = How many "8"s were predicted as "3"

**Cell 49: Visualizing Confusion Matrix**

```python
df_cm = pd.DataFrame(confusion_array, range(10), range(10))
sn.heatmap(df_cm, annot=True, annot_kws={"size": 10})
```

**Creates a color-coded heatmap:**
- **Bright colors** (high numbers): Correct predictions on diagonal
- **Dark colors** (low numbers): Few errors off diagonal
- **Numbers displayed**: Exact count for each cell

**Common observations:**
- Digit "1" often has highest accuracy (simple shape)
- Digits "3" and "5" sometimes confused (similar curves)
- Digits "4" and "9" sometimes confused (similar features)
- Digits "7" and "9" sometimes confused (overlapping strokes)

---

## Testing with Custom Handwritten Digits

This section demonstrates using the trained model on real handwritten digits. This is where we encountered and solved a critical bug!

**Cell 51: Loading Custom Images**

```python
from PIL import Image, ImageOps
import os
import glob

image_folder = 'digits-img'
image_paths = sorted(glob.glob(os.path.join(image_folder, '*.png')))
```

**What happens:**
1. Import PIL for image processing
2. Define folder containing custom digit images
3. Use `glob` to find all PNG files in folder
4. Sort them alphabetically (zero.png, one.png, ..., nine.png)

**Processing Each Image:**

```python
for img_path in image_paths:
    filename = os.path.basename(img_path)
    img = Image.open(img_path).convert('L')
    img = img.resize((28, 28))
    img_array = np.array(img) / 255.0
    img_array = 1.0 - img_array
    threshold = 0.5
    img_array = np.where(img_array > threshold, 1.0, 0.0)
```

**Step-by-step preprocessing:**

### Step 1: Load and Convert to Grayscale
```python
img = Image.open(img_path).convert('L')
```
- Opens the PNG file
- **`convert('L')`**: Converts to grayscale
- **'L'** mode = Luminance (grayscale, 0-255)

### Step 2: Resize to MNIST Size
```python
img = img.resize((28, 28))
```
- MNIST images are 28Ã—28 pixels
- Our custom images might be larger (e.g., 100Ã—100)
- **Resizing** ensures matching dimensions

### Step 3: Normalize to 0-1 Range
```python
img_array = np.array(img) / 255.0
```
- Converts PIL image to NumPy array
- Divides by 255 to get values between 0.0 and 1.0
- **Example**: 255 (white) â†’ 1.0, 0 (black) â†’ 0.0

### Step 4: Invert Colors
```python
img_array = 1.0 - img_array
```
**Why invert?**
- MNIST: White digits on black background (digit=1.0, background=0.0)
- Our images: Black digits on white background (digit=0.0, background=1.0)
- **Inversion**: Swaps them to match MNIST format

**Before inversion:**
- Background: 1.0 (white)
- Digit: 0.0 (black)

**After inversion:**
- Background: 0.0 (black)
- Digit: 1.0 (white)

### Step 5: CRITICAL FIX - Thresholding
```python
threshold = 0.5
img_array = np.where(img_array > threshold, 1.0, 0.0)
```

**This was THE solution to our bug!**

**The Problem We Had:**
- After inversion, backgrounds were ~0.27-0.29 (gray, not black)
- Digits were ~0.73 (gray, not white)
- MNIST backgrounds are exactly 0.0 (pure black)
- MNIST digits are 0.9-1.0 (nearly white)

**Why This Mismatch Broke Predictions:**
- Model trained on binary-like images (mostly 0.0 and 1.0)
- Received grayscale images (lots of 0.27-0.29 values)
- **Distribution shift**: Model couldn't recognize the patterns
- **Result**: All predictions were "3" regardless of actual digit

**How Thresholding Fixes It:**
- **`np.where(img_array > 0.5, 1.0, 0.0)`**: Binary decision
- If pixel > 0.5: Set to 1.0 (pure white)
- If pixel â‰¤ 0.5: Set to 0.0 (pure black)
- **Result**: Perfect match to MNIST format

**Visual Example:**
```
Before threshold: [0.27, 0.28, 0.73, 0.74, 0.29, 0.72]
After threshold:  [0.0,  0.0,  1.0,  1.0,  0.0,  1.0]
```

**Cell 52: Making Predictions**

```python
for filename, img_array in processed_images.items():
    img_tensor = torch.from_numpy(img_array).float().view(1, -1)
    
    with torch.no_grad():
        output = mymodel(img_tensor)
        probabilities = torch.exp(output)
        predicted_digit = torch.argmax(probabilities, dim=1).item()
        confidence = probabilities[0, predicted_digit].item() * 100
```

**Prediction process:**

1. **Convert to tensor**: `torch.from_numpy(img_array).float()`
   - NumPy array â†’ PyTorch tensor
   - Ensure float32 data type

2. **Flatten**: `.view(1, -1)`
   - Reshape from 28Ã—28 to 1Ã—784
   - **1**: Batch size of 1 (single image)
   - **-1**: Auto-calculate (784)

3. **Forward pass**: `mymodel(img_tensor)`
   - Process through neural network
   - Returns log probabilities for each digit (0-9)

4. **Convert to probabilities**: `torch.exp(output)`
   - Model outputs log probabilities
   - **`exp()`** converts back to regular probabilities
   - **Example**: log(-0.1) â†’ exp(-0.1) = 0.90 = 90%

5. **Get prediction**: `torch.argmax(probabilities, dim=1)`
   - Finds index with highest probability
   - **Example**: If index 7 is highest â†’ predicted digit is "7"

6. **Calculate confidence**: `probabilities[0, predicted_digit] * 100`
   - Extract probability of predicted digit
   - Convert to percentage
   - **Example**: 0.9823 â†’ 98.23%

**Cell 53: Visualization Grid**

```python
fig, axes = plt.subplots(rows, cols, figsize=(15, 3 * rows))
for idx, result in enumerate(results):
    axes[idx].imshow(img_array, cmap='gray')
    axes[idx].set_title(f"{filename}\nPrediction: {predicted}\nConfidence: {confidence:.1f}%")
```

**Creates visual output:**
- Grid of images (5 columns)
- Each image shows:
  - Filename (e.g., "three.png")
  - Predicted digit (e.g., "3")
  - Confidence percentage (e.g., "99.8%")

**Cell 54: Probability Distributions**

```python
for result in results:
    sorted_indices = np.argsort(probs)[::-1]
    for idx in sorted_indices[:5]:
        prob_percent = probs[idx] * 100
        bar = 'â–ˆ' * int(prob_percent / 5)
        marker = 'ðŸ‘‰' if idx == predicted else '  '
        print(f"{marker} Digit {idx}: {prob_percent:6.2f}% {bar}")
```

**Example output:**
```
ðŸ“„ three.png (Predicted: 3)
--------------------------------------------------
ðŸ‘‰ Digit 3:  99.82% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
   Digit 5:   0.12% 
   Digit 8:   0.04% 
   Digit 2:   0.01% 
   Digit 9:   0.01% 
```

**What this shows:**
- Top 5 most likely digits
- **ðŸ‘‰** marks the predicted digit
- **â–ˆ bars**: Visual representation of probability
- **Interpretation**: Model is very confident (99.82% for digit 3)

**After all fixes, final result:**
- **100% accuracy** on all 10 custom digits!
- Average confidence: >95%
- Model correctly recognizes: zero, one, two, ..., nine

---

## Key Lessons and Debugging Journey

### The Critical Bug and Its Solution

**Initial Problem:**
- Model achieved 97.59% accuracy on MNIST test set
- BUT predicted ALL custom images as "3"
- Even obviously different digits (like "0" or "1") predicted as "3"
- Extremely frustrating and confusing!

**Debugging Process:**

**Step 1: Added Diagnostic Cells**
- Compared MNIST image preprocessing with custom images
- Printed pixel value statistics (min, max, mean)
- Visualized both before/after inversion

**Step 2: Discovery**
```python
MNIST background: 0.000 (pure black)
Custom background: 0.275-0.294 (gray!)
```

**Root Cause:**
- After color inversion, custom images had gray backgrounds
- MNIST has pure black backgrounds
- Neural networks are sensitive to pixel value distributions
- **Distribution mismatch** = failed predictions

**Step 3: The Solution**
- Added thresholding: `np.where(img_array > 0.5, 1.0, 0.0)`
- Binarizes all pixels (0.0 or 1.0 only)
- Matches MNIST's binary-like distribution
- **Result**: 100% accuracy achieved!

### Important Lessons Learned

**1. Preprocessing Consistency is Critical**
- Input images MUST match training data format exactly
- Not just size and color, but pixel value distribution
- High test accuracy doesn't guarantee real-world performance

**2. Always Compare with Training Data**
- When predictions fail, compare your inputs to training examples
- Visualize distributions, not just images
- Small differences (0.27 vs 0.0) can break everything

**3. Debug Systematically**
- Add diagnostic cells to inspect intermediate steps
- Print statistics (min, max, mean, standard deviation)
- Compare successful vs. failing cases

**4. Neural Networks Learn Patterns, Not Concepts**
- Model learned "black backgrounds + white strokes = digits"
- When given "gray backgrounds," it failed
- Didn't truly "understand" digits like humans do

**5. Thresholding/Binarization as Preprocessing**
- Converts grayscale to binary
- Removes noise and variations
- Makes input more similar to training data
- Common technique in digit recognition

### Performance Metrics Discussion

**Why Accuracy is Appropriate for MNIST:**
- **Balanced dataset**: ~1,000 images per digit (0-9)
- **Equal costs**: Misclassifying "3" as "8" is equally bad as "1" as "7"
- **Single metric**: Easy to understand and compare

**When to Use Other Metrics:**

**Precision**: Use when false positives are costly
- Medical diagnosis (predicting disease when healthy)
- Spam detection (marking important email as spam)

**Recall**: Use when false negatives are costly
- Cancer screening (missing actual cancer cases)
- Fraud detection (missing actual fraud)

**F1-Score**: Use when class imbalance exists
- Rare event detection (1% positive class)
- Balances precision and recall

**For MNIST:**
- All classes equally important â†’ Accuracy sufficient
- No class imbalance â†’ F1 not needed
- No asymmetric costs â†’ Precision/Recall not prioritized

### Final Results Summary

**Model Performance:**
- Architecture: 784 â†’ 120 â†’ 84 â†’ 10 (MultilayerPerceptron)
- Training accuracy: 99.24%
- Test accuracy: 97.59%
- Parameters: 105,214
- Training time: ~10 minutes (10 epochs)

**Custom Digit Recognition:**
- Images processed: 10 (zero.png through nine.png)
- Accuracy: 100% (after preprocessing fix)
- Average confidence: >95%

**Key Success Factors:**
1. Proper architecture (sufficient hidden layers)
2. Appropriate activation functions (ReLU + Log Softmax)
3. Good optimizer (Adam with lr=0.001)
4. Sufficient training (10 epochs)
5. **CRITICAL**: Correct preprocessing (thresholding)

---

## Conclusion

This notebook demonstrates the complete pipeline of deep learning:
1. **Data Loading**: Downloading and preparing MNIST dataset
2. **Data Processing**: Batching, shuffling, transforming
3. **Model Architecture**: Designing neural network structure
4. **Training**: Forward pass, loss calculation, backpropagation
5. **Evaluation**: Testing on unseen data, confusion matrix
6. **Real-World Application**: Using model on custom images
7. **Debugging**: Identifying and fixing preprocessing issues

**The most important takeaway**: Deep learning isn't just about building models. It's about understanding data, preprocessing correctly, training effectively, and debugging systematically when things don't work as expected. The gray background bug taught us that even small differences in input format can completely break a well-trained model!
